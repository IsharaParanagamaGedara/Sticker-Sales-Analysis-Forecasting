{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":85723,"databundleVersionId":10652996,"sourceType":"competition"}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sulaniishara/sticker-sales-analysis-forecasting?scriptVersionId=221258267\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<!-- Include Google Fonts for a modern font -->\n<link href=\"https://fonts.googleapis.com/css2?family=Roboto:wght@700&display=swap\" rel=\"stylesheet\">\n\n<div style=\"border-radius: 15px; border: 2px solid #6A1B9A; padding: 20px; \n           background: linear-gradient(135deg, #6610f2, #ff99cc, #ffa07a, #ffff00); \n           text-align: center; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.5);\">\n    <h1 style=\"color: #ffffff; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7); \n               font-weight: bold; margin-bottom: 10px; font-size: 36px; \n               font-family: 'Roboto', sans-serif; line-height: 1.2;\">\n        📅 Sticker Sales Analysis & Forecasting 📈\n    </h1>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"#### **📊 Data Description**  \nThe dataset for this project consists of historical sales data for various products sold by a retailer. The data includes the following key components:\n\n1. **📂 Training Data (`train.csv`)**  \n   - Contains historical sales records used for training the model. Key columns include:  \n     - `id`: 🆔 Unique identifier for each record.  \n     - `date`: 📅 The date when sales were recorded.  \n     - `country`: 🌍 The country where the sales occurred (e.g., Canada, Finland, Italy).  \n     - `store`: 🏬 The store type (e.g., Discount Stickers, Stickers for Less).  \n     - `product`: 🛍️ The product type (e.g., Holographic Goose, Kaggle).  \n     - `num_sold`: 🔢 The number of units sold (target variable).  \n\n2. **📂 Test Data (`test.csv`)**  \n   - Contains future sales records without the `num_sold` column. The objective is to predict the values for this column.  \n\n3. **📄 Sample Submission File (`sample_submission.csv`)**  \n   - Provides the expected format for the submission, with `id` and `num_sold` columns. The predictions will replace the `num_sold` column.  \n\n4. **📊 Supplementary Data**  \n   - Ratios and weights such as day-of-week ratios, GDP per capita adjustments, and store weights are used to model and disaggregate sales trends.  \n\n#### **🎯 Objective**  \nThe primary objective of this project is to **forecast the number of products sold (`num_sold`) for each record in the test dataset**. This involves:  \n\n1. **🔍 Understanding Sales Patterns**  \n   Analyzing historical sales data to identify patterns influenced by:  \n   - 📆 Seasonality (e.g., holidays, weekends).  \n   - 🏢 Store types and country-specific trends.  \n   - 📈 Product demand variations.  \n\n2. **🛠️ Building a Robust Forecasting Model**  \n   Using techniques like:  \n   - 🔄 Imputation for handling missing data in historical records.  \n   - 📊 Aggregation and disaggregation to model total and individual sales accurately.  \n   - ⚖️ Ratio-based adjustments for finer granularity (e.g., GDP per capita, day-of-week effects).  \n\n3. **💡 Providing Actionable Insights**  \n   The final model will:  \n   - 📦 Help optimize inventory management and resource allocation across stores and regions.  \n   - 📊 Provide actionable insights into market demand trends for better strategic planning.  \n\nThrough the accurate prediction of future sales, this project aims to support **data-driven decision-making** 🧠 and enhance **profitability** 💰.","metadata":{}},{"cell_type":"markdown","source":"## Importing Required Libraries\n","metadata":{}},{"cell_type":"code","source":"# Importing necessary libraries\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport seaborn as sns\nfrom scipy.signal import find_peaks\nimport requests\nimport plotly.express as px\nimport plotly.io as pio\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:20:58.748023Z","iopub.execute_input":"2025-01-24T09:20:58.748326Z","iopub.status.idle":"2025-01-24T09:21:00.183202Z","shell.execute_reply.started":"2025-01-24T09:20:58.748297Z","shell.execute_reply":"2025-01-24T09:21:00.181608Z"},"_kg_hide-input":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading and Verifying the Datasets","metadata":{}},{"cell_type":"code","source":"# Load the datasets\ntrain_df = pd.read_csv(\"/kaggle/input/playground-series-s5e1/train.csv\", parse_dates=[\"date\"])\ntest_df = pd.read_csv(\"/kaggle/input/playground-series-s5e1/test.csv\", parse_dates=[\"date\"])\n# sample_df = pd.read_csv(\"/kaggle/input/playground-series-s5e1/sample_submission.csv\")\n\n# Verify shapes\nprint(\"Train Data Shape:\", train_df.shape)\nprint(\"Test Data Shape:\", test_df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:00.185309Z","iopub.execute_input":"2025-01-24T09:21:00.186058Z","iopub.status.idle":"2025-01-24T09:21:00.548909Z","shell.execute_reply.started":"2025-01-24T09:21:00.186015Z","shell.execute_reply":"2025-01-24T09:21:00.547311Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Displaying Sample Data","metadata":{}},{"cell_type":"code","source":"# Display sample data\nprint(\"Training Dataset: \\n\")\ndisplay(train_df.head(10))\nprint('\\n')\nprint(\"Test Dataset: \\n\")\ndisplay(test_df.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:00.550748Z","iopub.execute_input":"2025-01-24T09:21:00.551114Z","iopub.status.idle":"2025-01-24T09:21:00.583876Z","shell.execute_reply.started":"2025-01-24T09:21:00.551081Z","shell.execute_reply":"2025-01-24T09:21:00.582158Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset Information and Summary","metadata":{}},{"cell_type":"code","source":"# Display information for the training dataset\nprint(\"Training Dataset Information: \\n\")\ntrain_info = train_df.info()\ndisplay(train_info)\nprint('\\n')\n# Display information for the test dataset\nprint(\"Test Dataset Information: \\n\")\ntest_info = test_df.info()\ndisplay(test_info)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:00.584741Z","iopub.execute_input":"2025-01-24T09:21:00.585047Z","iopub.status.idle":"2025-01-24T09:21:00.673454Z","shell.execute_reply.started":"2025-01-24T09:21:00.58502Z","shell.execute_reply":"2025-01-24T09:21:00.671777Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Analyzing Unique Values in Columns","metadata":{}},{"cell_type":"code","source":"# Display the number of unique values in each column for train_df\nprint(\"\\nUnique Values in Each Column (Train Data):\")\nprint(train_df.nunique())\n\n# Display the lists of numerical and categorical columns in train_df\nnon_numerical_columns_train = train_df.select_dtypes(include=['object']).columns.tolist()\nprint(\"\\nCategorical Columns (Train Data):\", non_numerical_columns_train)\n\n# Display unique values for each categorical column in train_df\nfor col in non_numerical_columns_train:\n    print(f\"\\nColumn: {col}\")\n    print(f\"Unique Values: {train_df[col].unique()}\")\n\n# Display the number of unique values in each column for test_df\nprint(\"\\nUnique Values in Each Column (Test Data):\")\nprint(test_df.nunique())\n\n# Display the lists of numerical and categorical columns in test_df\nnon_numerical_columns_test = test_df.select_dtypes(include=['object']).columns.tolist()\nprint(\"\\nCategorical Columns (Test Data):\", non_numerical_columns_test)\n\n# Display unique values for each categorical column in test_df\nfor col in non_numerical_columns_test:\n    print(f\"\\nColumn: {col}\")\n    print(f\"Unique Values: {test_df[col].unique()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:00.67462Z","iopub.execute_input":"2025-01-24T09:21:00.675233Z","iopub.status.idle":"2025-01-24T09:21:00.84757Z","shell.execute_reply.started":"2025-01-24T09:21:00.675182Z","shell.execute_reply":"2025-01-24T09:21:00.846062Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Insights from Data Exploration**\n\n#### Dataset Overview\n\n- **Training Data**: The training dataset comprises **230,130 entries** and **6 columns**: `id`, `date`, `country`, `store`, `product`, and `num_sold`. Notably, the `num_sold` column has missing values, with only **221,259 non-null entries**.\n- **Test Data**: The test dataset consists of **98,550 entries** and **5 columns**: `id`, `date`, `country`, `store`, and `product`.\n\n#### Unique Values and Categorical Columns\n\n- **Unique Values**:\n  - Both datasets share the same unique values for the columns `country`, `store`, and `product`.\n    - **`country`**: 6 unique values — *Canada, Finland, Italy, Kenya, Norway, Singapore*.\n    - **`store`**: 3 unique values — *Discount Stickers, Stickers for Less, Premium Sticker Mart*.\n    - **`product`**: 5 unique values — *Holographic Goose, Kaggle, Kaggle Tiers, Kerneler, Kerneler Dark Mode*.\n\n- **Categorical Columns**:\n  - The categorical columns in both datasets are **`country`**, **`store`**, and **`product`**.\n\n#### Missing Values\n\n- **Training Data**: The `num_sold` column has missing values, with approximately **8,871 entries** lacking data.\n","metadata":{}},{"cell_type":"markdown","source":"## Fetching GDP Per Capita for a Country and Year","metadata":{}},{"cell_type":"code","source":"# Function to fetch GDP per capita for a given country and year\ndef get_gdp_per_capita(alpha3, year):\n    \"\"\"\n    Fetch GDP per capita for a specific country and year from the World Bank API.\n    \n    \"\"\"\n    url = f'https://api.worldbank.org/v2/country/{alpha3}/indicator/NY.GDP.PCAP.CD?date={year}&format=json'\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  \n        data = response.json()\n        return data[1][0]['value'] if data[1] else None  \n    except (requests.RequestException, KeyError, IndexError) as e:\n        print(f\"Error fetching data for {alpha3} in {year}: {e}\")\n        return None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:00.848722Z","iopub.execute_input":"2025-01-24T09:21:00.849165Z","iopub.status.idle":"2025-01-24T09:21:00.856402Z","shell.execute_reply.started":"2025-01-24T09:21:00.849095Z","shell.execute_reply":"2025-01-24T09:21:00.854797Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Normalizing GDP Ratios","metadata":{}},{"cell_type":"code","source":"# Function to create a DataFrame of GDP ratios\ndef create_gdp_dataframe(alpha3s, years, country_names):\n    \"\"\"\n    Create a DataFrame of normalized GDP per capita ratios for multiple countries and years.\n    \n    \"\"\"\n    # Fetch GDP data for all countries and years\n    gdp_data = [\n        [get_gdp_per_capita(alpha3, year) for year in years]\n        for alpha3 in alpha3s\n    ]\n    \n    # Create a DataFrame with countries as rows and years as columns\n    gdp_df = pd.DataFrame(gdp_data, index=country_names, columns=years)\n    \n    # Normalize GDP values by dividing by the column sum (yearly total)\n    gdp_df = gdp_df / gdp_df.sum(axis=0)\n    \n    # Reshape the DataFrame into long format\n    gdp_df = gdp_df.reset_index().rename(columns={'index': 'country'})\n    gdp_df = gdp_df.melt(id_vars=['country'], var_name='year', value_name='ratio')\n    \n    return gdp_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:00.860591Z","iopub.execute_input":"2025-01-24T09:21:00.860934Z","iopub.status.idle":"2025-01-24T09:21:00.888503Z","shell.execute_reply.started":"2025-01-24T09:21:00.860906Z","shell.execute_reply":"2025-01-24T09:21:00.887246Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Adjusting GDP Ratios for Specific Countries","metadata":{}},{"cell_type":"code","source":"# Function to adjust ratios for specific countries\ndef adjust_ratios(gdp_df, adjustments):\n    \"\"\"\n    Adjust GDP ratios for specific countries based on custom rules.\n    \n    \"\"\"\n    adjusted_df = gdp_df.copy()\n    for country, adjustment in adjustments.items():\n        adjusted_df.loc[adjusted_df['country'] == country, 'ratio'] -= adjustment\n    return adjusted_df\n\nif __name__ == \"__main__\":\n    # Define input parameters\n    alpha3s = ['CAN', 'FIN', 'ITA', 'KEN', 'NOR', 'SGP']\n    years = range(2010, 2020)\n    country_names = np.sort(['Canada', 'Finland', 'Italy', 'Kenya', 'Norway', 'Singapore'])  # Example list\n    \n    # Create the GDP DataFrame\n    gdp_ratios_df = create_gdp_dataframe(alpha3s, years, country_names)\n    \n    # Adjust Kenya's ratio by subtracting 0.0007\n    adjustments = {'Kenya': 0.0007}\n    gdp_per_capita_filtered_ratios_df = adjust_ratios(gdp_ratios_df, adjustments)\n    \n    print(gdp_per_capita_filtered_ratios_df.head(6))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:00.89036Z","iopub.execute_input":"2025-01-24T09:21:00.890726Z","iopub.status.idle":"2025-01-24T09:21:13.339879Z","shell.execute_reply.started":"2025-01-24T09:21:00.890683Z","shell.execute_reply":"2025-01-24T09:21:13.338799Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualizing GDP Ratios with a Choropleth Map","metadata":{}},{"cell_type":"code","source":"pio.renderers.default = 'iframe'\n\n# Filter for a specific year, e.g., 2010\ngdp_2010 = gdp_per_capita_filtered_ratios_df[gdp_per_capita_filtered_ratios_df['year'] == 2010]\n\n# Plot choropleth map\nfig = px.choropleth(\n    gdp_2010,\n    locations='country',\n    locationmode='country names',\n    color='ratio',\n    hover_name='country',\n    title='Normalized GDP Per Capita Ratios (2010)',\n    color_continuous_scale=px.colors.sequential.Plasma\n)\nfig.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:13.341076Z","iopub.execute_input":"2025-01-24T09:21:13.341406Z","iopub.status.idle":"2025-01-24T09:21:13.911385Z","shell.execute_reply.started":"2025-01-24T09:21:13.341376Z","shell.execute_reply":"2025-01-24T09:21:13.910323Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Heatmap of Missing Values in the Training Dataset","metadata":{}},{"cell_type":"code","source":"# Visualize missing values with a heatmap\nplt.figure(figsize=(10, 4))\nsns.heatmap(train_df.isnull(), cbar=False, cmap=\"plasma\")\nplt.title(\"Heatmap of Missing Values in Training Dataset\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:13.912738Z","iopub.execute_input":"2025-01-24T09:21:13.913095Z","iopub.status.idle":"2025-01-24T09:21:15.579095Z","shell.execute_reply.started":"2025-01-24T09:21:13.913065Z","shell.execute_reply":"2025-01-24T09:21:15.578007Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* The heatmap highlights missing values in the training dataset, primarily in the num_sold column. This indicates that some rows lack the target variable, which must be handled carefully during preprocessing.","metadata":{}},{"cell_type":"markdown","source":"## Observing Missing Values in the num_sold Column","metadata":{}},{"cell_type":"code","source":"# Count of missing and non-missing values\nmissing_count = train_df['num_sold'].isnull().sum()\nnon_missing_count = len(train_df) - missing_count\n\n# Define colors using the Plasma color palette\ncolors = [plt.cm.plasma(0.1), plt.cm.plasma(0.7)]\n\n# Create subplots: one for the bar chart and one for the donut chart\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\nax1.bar(['Non-missing', 'Missing'], [non_missing_count, missing_count], color=colors)\nax1.set_title('Distribution of Missing Values in num_sold')\nax1.set_ylabel('Count')\nax1.grid(True, linestyle='--', alpha=0.6)\n\nfor i, count in enumerate([non_missing_count, missing_count]):\n    ax1.text(i, count + 0.5, str(count), ha='center', va='bottom')\n\nlabels = ['Non-missing', 'Missing']\nsizes = [non_missing_count, missing_count]\nax2.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors, startangle=90,\n        wedgeprops=dict(width=0.3))  \nax2.axis('equal')  \nax2.set_title('Proportion of Missing Values in num_sold')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:15.58021Z","iopub.execute_input":"2025-01-24T09:21:15.580526Z","iopub.status.idle":"2025-01-24T09:21:15.915403Z","shell.execute_reply.started":"2025-01-24T09:21:15.580497Z","shell.execute_reply":"2025-01-24T09:21:15.914213Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Observational Insights\n\n1. **Missing vs. Non-Missing Counts**:\n   - **Non-Missing Values**: 221,259 entries (96.1% of the dataset).\n   - **Missing Values**: 8,871 entries (3.9% of the dataset).\n\nThe majority of the `num_sold` column is populated with valid data, with only a small fraction of rows containing missing values.\n","metadata":{}},{"cell_type":"markdown","source":"## Temporal Analysis of Missing Values Over Time","metadata":{}},{"cell_type":"code","source":"# Create a new column to indicate missingness\ntrain_df['num_sold_missing'] = train_df['num_sold'].isnull()\n\n# Group by date and calculate the percentage of missing values\nmissing_by_date = train_df.groupby('date')['num_sold_missing'].mean()\n\nplt.figure(figsize=(12, 4))\nmissing_by_date.plot(kind='line', color=plt.cm.plasma(0.2), alpha=0.7)\nplt.title(\"Trend of Missing 'num_sold' Values Over Time\")\nplt.xlabel('Date')\nplt.ylabel('Missing Value Percentage')\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\nprint(missing_by_date.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:15.916504Z","iopub.execute_input":"2025-01-24T09:21:15.916836Z","iopub.status.idle":"2025-01-24T09:21:16.448111Z","shell.execute_reply.started":"2025-01-24T09:21:15.916805Z","shell.execute_reply":"2025-01-24T09:21:16.444754Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* The time-series plot shows relatively consistent missingness, with occasional peaks, indicating certain dates may have experienced more missing entries.","metadata":{}},{"cell_type":"markdown","source":"## Analysis of Missing Values by Country","metadata":{}},{"cell_type":"code","source":"# Count missing values by country\nmissing_by_country = train_df[train_df['num_sold_missing']]['country'].value_counts()\n\nplt.figure(figsize=(12, 4))\nmissing_by_country.plot(kind='bar', color=plt.cm.plasma(0.8))\nplt.title(\"Missing 'num_sold' Values by Country\")\nplt.xlabel(\"Country\")\nplt.ylabel(\"Count of Missing Values\")\nplt.xticks(rotation=45)\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\nprint(missing_by_country.describe())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:16.449624Z","iopub.execute_input":"2025-01-24T09:21:16.45017Z","iopub.status.idle":"2025-01-24T09:21:16.64927Z","shell.execute_reply.started":"2025-01-24T09:21:16.450096Z","shell.execute_reply":"2025-01-24T09:21:16.64778Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The plot shows the count of missing values for the \"num_sold\" column across two countries: Kenya and Canada.\nKenya has a significantly higher number of missing values compared to Canada.\n\n**Specific Insights:**\n\n* **Kenya:** This country has the highest number of missing values for \"num_sold\".\n* **Canada:** This country has a lower number of missing values compared to Kenya.","metadata":{}},{"cell_type":"markdown","source":"## Analysis of Missing Values by Store","metadata":{}},{"cell_type":"code","source":"# Count missing values by store\nmissing_by_store = train_df[train_df['num_sold_missing']]['store'].value_counts()\n\nplt.figure(figsize=(12, 4))\nmissing_by_store.plot(kind='bar', color=plt.cm.plasma(0.4))\nplt.title(\"Missing 'num_sold' Values by Store\")\nplt.xlabel(\"Store\")\nplt.ylabel(\"Count of Missing Values\")\nplt.xticks(rotation=45)\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\nprint(missing_by_store.describe())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:16.65053Z","iopub.execute_input":"2025-01-24T09:21:16.650976Z","iopub.status.idle":"2025-01-24T09:21:16.878581Z","shell.execute_reply.started":"2025-01-24T09:21:16.650933Z","shell.execute_reply":"2025-01-24T09:21:16.87725Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The plot shows the count of missing values for the \"num_sold\" column across three different stores: Discount Stickers, Stickers for Less, and Premium Sticker Mart.\nThere is a significant variation in the number of missing values across the stores.\n\n**Specific Insights:**\n\n* **Discount Stickers:** This store has the highest number of missing values for \"num_sold\".\n* **Stickers for Less:** This store has the second-highest number of missing values.\n* **Premium Sticker Mart:** This store has the lowest number of missing values.","metadata":{}},{"cell_type":"markdown","source":"## Analysis of Missing Values by Product","metadata":{}},{"cell_type":"code","source":"# Count missing values by product\nmissing_by_product = train_df[train_df['num_sold_missing']]['product'].value_counts()\n\nplt.figure(figsize=(12, 4))\nmissing_by_product.plot(kind='bar', color=plt.cm.plasma(0.5))\nplt.title(\"Missing 'num_sold' Values by Product\")\nplt.xlabel(\"Product\")\nplt.ylabel(\"Count of Missing Values\")\nplt.xticks(rotation=45)\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\nprint(missing_by_product.describe())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:16.87998Z","iopub.execute_input":"2025-01-24T09:21:16.880359Z","iopub.status.idle":"2025-01-24T09:21:17.149442Z","shell.execute_reply.started":"2025-01-24T09:21:16.880327Z","shell.execute_reply":"2025-01-24T09:21:17.147848Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The plot shows the count of missing values for the \"num_sold\" column across three different products: Holographic Goose, Kerneler, and Kerneler Dark Mode.\nThere is a significant variation in the number of missing values across the products.\n\n**Specific Insights:**\n\n* **Holographic Goose:** This product has the highest number of missing values for \"num_sold\".\n* **Kerneler:** This product has a much lower number of missing values compared to Holographic Goose.\n* **Kerneler Dark Mode:** This product has the least number of missing values, with only a very small number of missing values.","metadata":{}},{"cell_type":"markdown","source":"## Analyzing Missing Values by Country and Store","metadata":{}},{"cell_type":"code","source":"# Count missing values grouped by both country and store\nmissing_by_country_store = (\n    train_df[train_df['num_sold_missing']]\n    .groupby(['country', 'store'])['num_sold']\n    .size()\n    .reset_index(name='missing_count')\n)\n\nplt.figure(figsize=(14, 6))\n\n# Create a bar plot for missing values by store within each country\nsns.barplot(\n    data=missing_by_country_store,\n    x='store',\n    y='missing_count',\n    hue='country',\n    palette='plasma'\n)\n\nplt.title(\"Missing 'num_sold' Values by Store for Each Country\", fontsize=16)\nplt.xlabel(\"Store\", fontsize=12)\nplt.ylabel(\"Count of Missing Values\", fontsize=12)\nplt.xticks(rotation=45, fontsize=12)\nplt.legend(title='Country', fontsize=12, title_fontsize=12)\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:17.151184Z","iopub.execute_input":"2025-01-24T09:21:17.151947Z","iopub.status.idle":"2025-01-24T09:21:17.504495Z","shell.execute_reply.started":"2025-01-24T09:21:17.151882Z","shell.execute_reply":"2025-01-24T09:21:17.503098Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The plot visualizes the count of missing values for the \"num_sold\" column across three different stores (Discount Stickers, Premium Sticker Mart, Stickers for Less) and two countries (Canada and Kenya).\nThere is a noticeable variation in the number of missing values across stores and between the two countries.\n\n**Specific Insights:**\n\n* **Discount Stickers:** This store has the highest number of missing values for \"num_sold\" in both Canada and Kenya.\n* **Stickers for Less:** This store has a moderate number of missing values in both countries.\n* **Premium Sticker Mart:** This store has the lowest number of missing values in both countries.\n* **Country Comparison:**\n    * In general, Discount Stickers has a significantly higher number of missing values in Kenya compared to Canada.\n    * For Stickers for Less and Premium Sticker Mart, the difference in missing values between Canada and Kenya is less pronounced.","metadata":{}},{"cell_type":"markdown","source":"## Analyzing Missing Values by Country and Product","metadata":{}},{"cell_type":"code","source":"# Count missing values grouped by both country and product\nmissing_by_country_product = (\n    train_df[train_df['num_sold_missing']]\n    .groupby(['country', 'product'])['num_sold']\n    .size()\n    .reset_index(name='missing_count')\n)\n\nplt.figure(figsize=(14, 6))\n\n# Create a bar plot for missing values by product within each country\nsns.barplot(\n    data=missing_by_country_product,\n    x='product',\n    y='missing_count',\n    hue='country',\n    palette='plasma'\n)\n\nplt.title(\"Missing 'num_sold' Values by Product for Each Country\", fontsize=16)\nplt.xlabel(\"Product\", fontsize=12)\nplt.ylabel(\"Count of Missing Values\", fontsize=12)\nplt.xticks(rotation=45, fontsize=12)\nplt.legend(title='Country', fontsize=12, title_fontsize=12)\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:17.505541Z","iopub.execute_input":"2025-01-24T09:21:17.505852Z","iopub.status.idle":"2025-01-24T09:21:17.84404Z","shell.execute_reply.started":"2025-01-24T09:21:17.505826Z","shell.execute_reply":"2025-01-24T09:21:17.842586Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The plot visualizes the count of missing values for the \"num_sold\" column across three different products (Holographic Goose, Kerneler, Kerneler Dark Mode) and two countries (Canada and Kenya).\nThere is a significant variation in the number of missing values across products and between the two countries.\n\n**Specific Insights:**\n\n* **Holographic Goose:** This product has the highest number of missing values in both Canada and Kenya.\n* **Kerneler:** This product has a much lower number of missing values compared to Holographic Goose in both countries.\n* **Kerneler Dark Mode:** This product has the least number of missing values, with only a very small number of missing values in both countries.\n* **Country Comparison:**\n    * For Holographic Goose, the number of missing values is higher in Kenya compared to Canada.\n    * For Kerneler and Kerneler Dark Mode, the difference in missing values between Canada and Kenya is less pronounced.","metadata":{}},{"cell_type":"markdown","source":"## Analyzing Missing Values by Store and Product","metadata":{}},{"cell_type":"code","source":"# Count missing values grouped by both store and product\nmissing_by_store_product = (\n    train_df[train_df['num_sold_missing']]\n    .groupby(['store', 'product'])['num_sold']\n    .size()\n    .reset_index(name='missing_count')\n)\n\nplt.figure(figsize=(14, 6))\n\n# Create a bar plot for missing values by product within each store\nsns.barplot(\n    data=missing_by_store_product,\n    x='product',\n    y='missing_count',\n    hue='store',\n    palette='plasma'\n)\n\nplt.title(\"Missing 'num_sold' Values by Product for Each Store\", fontsize=16)\nplt.xlabel(\"Product\", fontsize=12)\nplt.ylabel(\"Count of Missing Values\", fontsize=12)\nplt.xticks(rotation=45, fontsize=12)\nplt.legend(title='Store', fontsize=12, title_fontsize=12)\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:17.845453Z","iopub.execute_input":"2025-01-24T09:21:17.845898Z","iopub.status.idle":"2025-01-24T09:21:18.216318Z","shell.execute_reply.started":"2025-01-24T09:21:17.845856Z","shell.execute_reply":"2025-01-24T09:21:18.215035Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The plot visualizes the count of missing values for the \"num_sold\" column across three different products (Holographic Goose, Kerneler, Kerneler Dark Mode) and three different stores (Discount Stickers, Premium Sticker Mart, Stickers for Less).\nThere is a significant variation in the number of missing values across products and stores.\n\n**Specific Insights:**\n\n* **Holographic Goose:** This product has the highest number of missing values across all stores. \n    * **Discount Stickers:** This store has the highest number of missing values for Holographic Goose.\n    * **Premium Sticker Mart:** This store has a moderate number of missing values for Holographic Goose.\n    * **Stickers for Less:** This store has a relatively low number of missing values for Holographic Goose.\n\n* **Kerneler:** This product has a much lower number of missing values compared to Holographic Goose across all stores.\n    * **Discount Stickers:** This store has the highest number of missing values for Kerneler.\n    * **Premium Sticker Mart:** This store has a very low number of missing values for Kerneler.\n    * **Stickers for Less:** This store has a very low number of missing values for Kerneler.\n\n* **Kerneler Dark Mode:** This product has the least number of missing values across all stores. \n    * **Discount Stickers:** This store has the highest number of missing values for Kerneler Dark Mode.\n    * **Premium Sticker Mart:** This store has a very low number of missing values for Kerneler Dark Mode.\n    * **Stickers for Less:** This store has a very low number of missing values for Kerneler Dark Mode.","metadata":{}},{"cell_type":"markdown","source":"### Dropping the Temporary Missingness Column","metadata":{}},{"cell_type":"code","source":"# Dropping specified column from train_data\ntrain_df = train_df.drop('num_sold_missing', axis=1)\n\n# Display the updated DataFrame to confirm the columns have been dropped\nprint(\"Updated train_data after dropping specified columns:\")\nprint(train_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:18.217439Z","iopub.execute_input":"2025-01-24T09:21:18.217775Z","iopub.status.idle":"2025-01-24T09:21:18.241238Z","shell.execute_reply.started":"2025-01-24T09:21:18.217741Z","shell.execute_reply":"2025-01-24T09:21:18.239426Z"},"_kg_hide-input":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Imputation of Missing Sales Data","metadata":{}},{"cell_type":"code","source":"# Create a copy of the DataFrame\ntrain_df_imputed = train_df.copy()\nprint(f\"Missing values remaining: {train_df_imputed['num_sold'].isna().sum()}\")\n\n# Extract the year from the date\ntrain_df_imputed[\"year\"] = train_df_imputed[\"date\"].dt.year\n\n# Loop through each year to perform imputation\nfor year in train_df_imputed[\"year\"].unique():\n    # Target ratio (Norway)\n    target_ratio = gdp_per_capita_filtered_ratios_df.loc[\n        (gdp_per_capita_filtered_ratios_df[\"year\"] == year) & \n        (gdp_per_capita_filtered_ratios_df[\"country\"] == \"Norway\"), \"ratio\"\n    ].values[0]\n\n    # Impute Time Series 1: Canada, Discount Stickers, Holographic Goose\n    current_ratio_can = gdp_per_capita_filtered_ratios_df.loc[\n        (gdp_per_capita_filtered_ratios_df[\"year\"] == year) & \n        (gdp_per_capita_filtered_ratios_df[\"country\"] == \"Canada\"), \"ratio\"\n    ].values[0]\n    ratio_can = current_ratio_can / target_ratio\n    train_df_imputed.loc[\n        (train_df_imputed[\"country\"] == \"Canada\") & \n        (train_df_imputed[\"store\"] == \"Discount Stickers\") & \n        (train_df_imputed[\"product\"] == \"Holographic Goose\") & \n        (train_df_imputed[\"year\"] == year), \n        \"num_sold\"\n    ] = (\n        train_df_imputed.loc[\n            (train_df_imputed[\"country\"] == \"Norway\") & \n            (train_df_imputed[\"store\"] == \"Discount Stickers\") & \n            (train_df_imputed[\"product\"] == \"Holographic Goose\") & \n            (train_df_imputed[\"year\"] == year), \n            \"num_sold\"\n        ] * ratio_can\n    ).values\n\n    # Impute Time Series 2-3: Canada, Premium Sticker Mart / Stickers for Less\n    for store in [\"Premium Sticker Mart\", \"Stickers for Less\"]:\n        current_ts = train_df_imputed.loc[\n            (train_df_imputed[\"country\"] == \"Canada\") & \n            (train_df_imputed[\"store\"] == store) & \n            (train_df_imputed[\"product\"] == \"Holographic Goose\") & \n            (train_df_imputed[\"year\"] == year)\n        ]\n        missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n        train_df_imputed.loc[\n            (train_df_imputed[\"country\"] == \"Canada\") & \n            (train_df_imputed[\"store\"] == store) & \n            (train_df_imputed[\"product\"] == \"Holographic Goose\") & \n            (train_df_imputed[\"year\"] == year) & \n            (train_df_imputed[\"date\"].isin(missing_ts_dates)), \n            \"num_sold\"\n        ] = (\n            train_df_imputed.loc[\n                (train_df_imputed[\"country\"] == \"Norway\") & \n                (train_df_imputed[\"store\"] == store) & \n                (train_df_imputed[\"product\"] == \"Holographic Goose\") & \n                (train_df_imputed[\"year\"] == year) & \n                (train_df_imputed[\"date\"].isin(missing_ts_dates)), \n                \"num_sold\"\n            ] * ratio_can\n        ).values\n\n    # Impute Time Series 4: Kenya, Discount Stickers, Holographic Goose\n    current_ratio_ken = gdp_per_capita_filtered_ratios_df.loc[\n        (gdp_per_capita_filtered_ratios_df[\"year\"] == year) & \n        (gdp_per_capita_filtered_ratios_df[\"country\"] == \"Kenya\"), \"ratio\"\n    ].values[0]\n    ratio_ken = current_ratio_ken / target_ratio\n    train_df_imputed.loc[\n        (train_df_imputed[\"country\"] == \"Kenya\") & \n        (train_df_imputed[\"store\"] == \"Discount Stickers\") & \n        (train_df_imputed[\"product\"] == \"Holographic Goose\") & \n        (train_df_imputed[\"year\"] == year), \n        \"num_sold\"\n    ] = (\n        train_df_imputed.loc[\n            (train_df_imputed[\"country\"] == \"Norway\") & \n            (train_df_imputed[\"store\"] == \"Discount Stickers\") & \n            (train_df_imputed[\"product\"] == \"Holographic Goose\") & \n            (train_df_imputed[\"year\"] == year), \n            \"num_sold\"\n        ] * ratio_ken\n    ).values\n\n    # Impute Time Series 5-6: Kenya, Premium Sticker Mart / Stickers for Less\n    for store in [\"Premium Sticker Mart\", \"Stickers for Less\"]:\n        current_ts = train_df_imputed.loc[\n            (train_df_imputed[\"country\"] == \"Kenya\") & \n            (train_df_imputed[\"store\"] == store) & \n            (train_df_imputed[\"product\"] == \"Holographic Goose\") & \n            (train_df_imputed[\"year\"] == year)\n        ]\n        missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n        train_df_imputed.loc[\n            (train_df_imputed[\"country\"] == \"Kenya\") & \n            (train_df_imputed[\"store\"] == store) & \n            (train_df_imputed[\"product\"] == \"Holographic Goose\") & \n            (train_df_imputed[\"year\"] == year) & \n            (train_df_imputed[\"date\"].isin(missing_ts_dates)), \n            \"num_sold\"\n        ] = (\n            train_df_imputed.loc[\n                (train_df_imputed[\"country\"] == \"Norway\") & \n                (train_df_imputed[\"store\"] == store) & \n                (train_df_imputed[\"product\"] == \"Holographic Goose\") & \n                (train_df_imputed[\"year\"] == year) & \n                (train_df_imputed[\"date\"].isin(missing_ts_dates)), \n                \"num_sold\"\n            ] * ratio_ken\n        ).values\n\n    # Impute Time Series 7: Kenya, Discount Stickers, Kerneler\n    current_ts = train_df_imputed.loc[\n        (train_df_imputed[\"country\"] == \"Kenya\") & \n        (train_df_imputed[\"store\"] == \"Discount Stickers\") & \n        (train_df_imputed[\"product\"] == \"Kerneler\") & \n        (train_df_imputed[\"year\"] == year)\n    ]\n    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n    train_df_imputed.loc[\n        (train_df_imputed[\"country\"] == \"Kenya\") & \n        (train_df_imputed[\"store\"] == \"Discount Stickers\") & \n        (train_df_imputed[\"product\"] == \"Kerneler\") & \n        (train_df_imputed[\"year\"] == year) & \n        (train_df_imputed[\"date\"].isin(missing_ts_dates)), \n        \"num_sold\"\n    ] = (\n        train_df_imputed.loc[\n            (train_df_imputed[\"country\"] == \"Norway\") & \n            (train_df_imputed[\"store\"] == \"Discount Stickers\") & \n            (train_df_imputed[\"product\"] == \"Kerneler\") & \n            (train_df_imputed[\"year\"] == year) & \n            (train_df_imputed[\"date\"].isin(missing_ts_dates)), \n            \"num_sold\"\n        ] * ratio_ken\n    ).values\n\n# Check for remaining missing values\nprint(f\"Missing values remaining after imputation: {train_df_imputed['num_sold'].isna().sum()}\")\n\n# Manual imputation for specific IDs\ntrain_df_imputed.loc[train_df_imputed[\"id\"] == 23719, \"num_sold\"] = 4\ntrain_df_imputed.loc[train_df_imputed[\"id\"] == 207003, \"num_sold\"] = 195\n\n# Final check for missing values\nprint(f\"Final missing values remaining: {train_df_imputed['num_sold'].isna().sum()}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:18.243149Z","iopub.execute_input":"2025-01-24T09:21:18.243528Z","iopub.status.idle":"2025-01-24T09:21:26.127655Z","shell.execute_reply.started":"2025-01-24T09:21:18.243497Z","shell.execute_reply":"2025-01-24T09:21:26.126367Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Detection of Duplicates","metadata":{}},{"cell_type":"code","source":"# Check for duplicate rows in the training dataset\ntrain_duplicates = train_df_imputed.duplicated().sum()\nprint(f\"\\nNumber of duplicate rows in the training dataset: {train_duplicates}\")\n\n# Check for duplicate rows in the test dataset\ntest_duplicates = test_df.duplicated().sum()\nprint(f\"Number of duplicate rows in the test dataset: {test_duplicates}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:26.12985Z","iopub.execute_input":"2025-01-24T09:21:26.13026Z","iopub.status.idle":"2025-01-24T09:21:26.282783Z","shell.execute_reply.started":"2025-01-24T09:21:26.130226Z","shell.execute_reply":"2025-01-24T09:21:26.280596Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Distribution of num_sold","metadata":{}},{"cell_type":"code","source":"# Custom colormap using Plasma\nplasma_cmap = cm.get_cmap(\"plasma\")\n\ndef visualize_num_sold_with_peaks(data, feature='num_sold'):\n    plt.figure(figsize=(12, 6))\n\n    plt.subplot(1, 2, 1)\n    ax = sns.histplot(data[feature], bins=30, kde=True, color=plasma_cmap(0.5))\n    plt.title(f'Histogram of {feature} with KDE', fontsize=12)\n    plt.xlabel(feature, fontsize=10)\n    plt.ylabel('Frequency', fontsize=10)\n    plt.grid(True, linestyle='--', alpha=0.6)\n\n    kde = sns.kdeplot(data[feature], ax=ax, color=plasma_cmap(0.7)).lines[0].get_data()\n    kde_x, kde_y = kde[0], kde[1]\n    peaks, _ = find_peaks(kde_y)\n\n    for peak_idx in peaks:\n        plt.plot(kde_x[peak_idx], kde_y[peak_idx], \"ro\")  \n\n    plt.subplot(1, 2, 2)\n    sns.boxplot(x=data[feature], color=plasma_cmap(0.5))\n    plt.title(f'Box Plot of {feature}', fontsize=12)\n    plt.xlabel(feature, fontsize=10)\n    plt.grid(True, linestyle='--', alpha=0.6)\n    plt.tight_layout()\n    plt.show()\n\nvisualize_num_sold_with_peaks(train_df_imputed, feature='num_sold')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:26.288721Z","iopub.execute_input":"2025-01-24T09:21:26.289085Z","iopub.status.idle":"2025-01-24T09:21:29.084174Z","shell.execute_reply.started":"2025-01-24T09:21:26.289055Z","shell.execute_reply":"2025-01-24T09:21:29.083045Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Distribution of country, store, and product","metadata":{}},{"cell_type":"code","source":"# Function to display bar plot and pie chart for categorical columns\ndef plot_categorical_distribution(data, column_name):\n    plasma_colors = sns.color_palette(\"plasma\", data[column_name].nunique())\n    \n    plt.figure(figsize=(12, 4))\n    \n    plt.subplot(1, 2, 1)\n    sns.countplot(y=column_name, data=data, palette=plasma_colors)\n    plt.title(f'Distribution of {column_name}', fontsize=12)\n    plt.xlabel('Count', fontsize=10)\n    plt.ylabel(column_name, fontsize=10)\n\n    ax = plt.gca()\n    for p in ax.patches:\n        count = int(p.get_width())\n        ax.annotate(f'{count}', \n                    (p.get_width() + 0.1, p.get_y() + p.get_height() / 2), \n                    ha='left', va='center', fontsize=10, color='black')\n    \n    sns.despine(left=True, bottom=True)\n    \n    # Pie chart for percentage distribution\n    plt.subplot(1, 2, 2)\n    data[column_name].value_counts().plot.pie(\n        autopct='%1.1f%%', \n        colors=plasma_colors, \n        startangle=90, \n        explode=[0.05] * data[column_name].nunique(), \n        shadow=True\n    )\n    plt.title(f'Percentage Distribution of {column_name}', fontsize=12)\n    plt.ylabel('')  \n\n    plt.tight_layout()\n    plt.show()\n\ncategorical_columns = ['country', 'store', 'product']\nfor column in categorical_columns:\n    plot_categorical_distribution(train_df_imputed, column)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:29.086106Z","iopub.execute_input":"2025-01-24T09:21:29.08645Z","iopub.status.idle":"2025-01-24T09:21:30.774094Z","shell.execute_reply.started":"2025-01-24T09:21:29.086423Z","shell.execute_reply":"2025-01-24T09:21:30.772277Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Distribution of num_sold across categorical features","metadata":{}},{"cell_type":"code","source":"def facetgrid_and_boxplot(data, categorical_column, target_column):\n    g = sns.FacetGrid(data, col=categorical_column, col_wrap=3, height=4, sharex=False, sharey=False, palette=\"plasma\")\n    g.map(sns.histplot, target_column, kde=False, bins=30, color=sns.color_palette(\"plasma\")[0])\n    g.set_titles(\"{col_name}\")\n    g.set_axis_labels(target_column, \"Frequency\")\n    g.fig.suptitle(f\"Distribution of {target_column} across unique values of {categorical_column}\", y=1.02, fontsize=16)\n    g.tight_layout()\n    plt.show()\n\n    plt.figure(figsize=(10, 4))\n    sns.boxplot(x=categorical_column, y=target_column, data=data, palette=\"plasma\")\n    plt.title(f\"Boxplot of {target_column} by {categorical_column}\", fontsize=12)\n    plt.xlabel(categorical_column, fontsize=10)\n    plt.ylabel(target_column, fontsize=10)\n    plt.grid(True, linestyle='--', alpha=0.6)\n    plt.tight_layout()\n    plt.show()\n\ntarget_column = \"num_sold\"\ncategorical_column = \"country\"\n\nfacetgrid_and_boxplot(train_df_imputed, categorical_column, target_column)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:30.77537Z","iopub.execute_input":"2025-01-24T09:21:30.775809Z","iopub.status.idle":"2025-01-24T09:21:34.440672Z","shell.execute_reply.started":"2025-01-24T09:21:30.775763Z","shell.execute_reply":"2025-01-24T09:21:34.43906Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def facetgrid_and_boxplot(data, categorical_column, target_column):\n    g = sns.FacetGrid(data, col=categorical_column, col_wrap=3, height=4, sharex=False, sharey=False, palette=\"plasma\")\n    g.map(sns.histplot, target_column, kde=False, bins=30, color=sns.color_palette(\"plasma\")[1])\n    g.set_titles(\"{col_name}\")\n    g.set_axis_labels(target_column, \"Frequency\")\n    g.fig.suptitle(f\"Distribution of {target_column} across unique values of {categorical_column}\", y=1.02, fontsize=16)\n    g.tight_layout()\n    plt.show()\n\n    plt.figure(figsize=(10, 4))\n    sns.boxplot(x=categorical_column, y=target_column, data=data, palette=\"plasma\")\n    plt.title(f\"Boxplot of {target_column} by {categorical_column}\", fontsize=12)\n    plt.xlabel(categorical_column, fontsize=10)\n    plt.ylabel(target_column, fontsize=10)\n    plt.grid(True, linestyle='--', alpha=0.6)\n    plt.tight_layout()\n    plt.show()\n\ntarget_column = \"num_sold\"\ncategorical_column = \"store\"\n\nfacetgrid_and_boxplot(train_df_imputed, categorical_column, target_column)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:34.442212Z","iopub.execute_input":"2025-01-24T09:21:34.44263Z","iopub.status.idle":"2025-01-24T09:21:36.646758Z","shell.execute_reply.started":"2025-01-24T09:21:34.442586Z","shell.execute_reply":"2025-01-24T09:21:36.645752Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def facetgrid_and_boxplot(data, categorical_column, target_column):\n    g = sns.FacetGrid(data, col=categorical_column, col_wrap=3, height=4, sharex=False, sharey=False, palette=\"plasma\")\n    g.map(sns.histplot, target_column, kde=False, bins=30, color=sns.color_palette(\"plasma\")[2])\n    g.set_titles(\"{col_name}\")\n    g.set_axis_labels(target_column, \"Frequency\")\n    g.fig.suptitle(f\"Distribution of {target_column} across unique values of {categorical_column}\", y=1.02, fontsize=16)\n    g.tight_layout()\n    plt.show()\n\n    plt.figure(figsize=(10, 4))\n    sns.boxplot(x=categorical_column, y=target_column, data=data, palette=\"plasma\")\n    plt.title(f\"Boxplot of {target_column} by {categorical_column}\", fontsize=12)\n    plt.xlabel(categorical_column, fontsize=10)\n    plt.ylabel(target_column, fontsize=10)\n    plt.grid(True, linestyle='--', alpha=0.6)\n    plt.tight_layout()\n    plt.show()\n\ntarget_column = \"num_sold\"\ncategorical_column = \"product\"\n\nfacetgrid_and_boxplot(train_df_imputed, categorical_column, target_column)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:36.64788Z","iopub.execute_input":"2025-01-24T09:21:36.648254Z","iopub.status.idle":"2025-01-24T09:21:39.905529Z","shell.execute_reply.started":"2025-01-24T09:21:36.648223Z","shell.execute_reply":"2025-01-24T09:21:39.904528Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Daily Sales Trends\nAggregating sales by date and visualizing daily sales trends","metadata":{}},{"cell_type":"code","source":"# Aggregate sales by date\ndaily_sales = train_df_imputed.groupby('date')['num_sold'].sum().reset_index()\n\nplt.figure(figsize=(12, 6))\nsns.lineplot(data=daily_sales, x='date', y='num_sold', color=sns.color_palette(\"plasma\")[0])\nplt.title('Daily Sales Trend', fontsize=14)\nplt.xlabel('Date', fontsize=12)\nplt.ylabel('Total Sales', fontsize=12)\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:39.906517Z","iopub.execute_input":"2025-01-24T09:21:39.906793Z","iopub.status.idle":"2025-01-24T09:21:40.329266Z","shell.execute_reply.started":"2025-01-24T09:21:39.90677Z","shell.execute_reply":"2025-01-24T09:21:40.328149Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Daily Sales Trends by Categories\nVisualizing daily sales trends for each category (country, store, product) ","metadata":{}},{"cell_type":"code","source":"categorical_columns = ['country', 'store', 'product']\n\n# Set up the subplots for daily sales trends\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(24, 8), sharey=True)\n\n# Loop through each category and create a subplot for daily trends\nfor i, category in enumerate(categorical_columns):\n    # Aggregate sales by date and category\n    category_sales_daily = train_df_imputed.groupby(['date', category])['num_sold'].sum().reset_index()\n    \n    sns.lineplot(\n        data=category_sales_daily,\n        x='date',\n        y='num_sold',\n        hue=category,\n        palette='plasma',\n        linewidth=1,\n        ax=axes[i]\n    )\n    \n    axes[i].set_title(f'Daily Sales Trend by {category.capitalize()}', fontsize=14)\n    axes[i].set_xlabel('Date', fontsize=12)\n    axes[i].set_ylabel('Total Sales', fontsize=12 if i == 0 else 0)  \n    axes[i].legend(title=category.capitalize(), fontsize=10)\n    axes[i].grid(True, linestyle='--', alpha=0.6)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:40.330365Z","iopub.execute_input":"2025-01-24T09:21:40.330717Z","iopub.status.idle":"2025-01-24T09:21:42.217139Z","shell.execute_reply.started":"2025-01-24T09:21:40.330685Z","shell.execute_reply":"2025-01-24T09:21:42.215808Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Contribution to Total Sales by Categories\nCalculating and visualizing weights (proportional contributions) for country, store, and product.","metadata":{}},{"cell_type":"code","source":"# Function to calculate weights of each country based on total sales\ndef calculate_country_weights(df):\n    \"\"\"\n    Calculate the weights of each country based on total sales.\n    \"\"\"\n    total_sales = df[\"num_sold\"].sum()\n    country_weights = df.groupby(\"country\")[\"num_sold\"].sum() / total_sales\n    return country_weights\n\n# Function to calculate weights of each store based on total sales\ndef calculate_store_weights(df):\n    \"\"\"\n    Calculate the weights of each store based on total sales.\n    \"\"\"\n    total_sales = df[\"num_sold\"].sum()\n    store_weights = df.groupby(\"store\")[\"num_sold\"].sum() / total_sales\n    return store_weights\n\n# Function to calculate weights of each product based on total sales\ndef calculate_product_weights(df):\n    \"\"\"\n    Calculate the weights of each product based on total sales.\n    \"\"\"\n    total_sales = df[\"num_sold\"].sum()\n    product_weights = df.groupby(\"product\")[\"num_sold\"].sum() / total_sales\n    return product_weights\n\n# Calculate weights\ncountry_weights = calculate_country_weights(train_df_imputed)\nstore_weights = calculate_store_weights(train_df_imputed)\nproduct_weights = calculate_product_weights(train_df_imputed)\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n# Plot Country Weights\nsns.barplot(x=country_weights.index, y=country_weights.values, palette='plasma', ax=axes[0])\naxes[0].set_title('Country Contribution to Total Sales', fontsize=12)\naxes[0].set_xlabel('Country', fontsize=12)\naxes[0].set_ylabel('Weight (Proportion of Total Sales)', fontsize=12)\naxes[0].tick_params(axis='x', rotation=45)\naxes[0].grid(axis='y', linestyle='--', alpha=0.6)\n\n# Plot Store Weights\nsns.barplot(x=store_weights.index, y=store_weights.values, palette='plasma', ax=axes[1])\naxes[1].set_title('Store Contribution to Total Sales', fontsize=12)\naxes[1].set_xlabel('Store', fontsize=12)\naxes[1].set_ylabel('Weight (Proportion of Total Sales)', fontsize=12)\naxes[1].tick_params(axis='x', rotation=45)\naxes[1].grid(axis='y', linestyle='--', alpha=0.6)\n\n# Plot Product Weights\nsns.barplot(x=product_weights.index, y=product_weights.values, palette='plasma', ax=axes[2])\naxes[2].set_title('Product Contribution to Total Sales', fontsize=12)\naxes[2].set_xlabel('Product', fontsize=12)\naxes[2].set_ylabel('Weight (Proportion of Total Sales)', fontsize=12)\naxes[2].tick_params(axis='x', rotation=45)\naxes[2].grid(axis='y', linestyle='--', alpha=0.6)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:42.218743Z","iopub.execute_input":"2025-01-24T09:21:42.219068Z","iopub.status.idle":"2025-01-24T09:21:42.932717Z","shell.execute_reply.started":"2025-01-24T09:21:42.219039Z","shell.execute_reply":"2025-01-24T09:21:42.931474Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def decompose(data, group_col, ax, colormap='plasma'):\n    \"\"\"\n    Decomposes and plots the fraction of sales for each group (e.g., store, product) over time\n    \n    \"\"\"\n    # Group data by date and group_col, then calculate total sales\n    grouped = data.groupby(['date', group_col])['num_sold'].sum().reset_index()\n    \n    # Calculate global totals by date\n    global_totals = data.groupby('date')['num_sold'].sum().reset_index()\n    global_totals.rename(columns={'num_sold': 'num_sold_global'}, inplace=True)\n    \n    # Merge grouped data with global totals\n    merged = grouped.merge(global_totals, on='date')\n    merged['fractions'] = merged['num_sold'] / merged['num_sold_global']\n    \n    unique_groups = np.sort(merged[group_col].unique())\n    colors = cm.get_cmap(colormap, len(unique_groups))\n    \n    for i, group in enumerate(unique_groups):\n        mask = merged[group_col] == group\n        ax.plot(\n            merged[mask]['date'], \n            merged[mask]['fractions'], \n            label=group, \n            color=colors(i)  \n        )\n    \n    ax.legend(bbox_to_anchor=(1, 1), title=group_col.capitalize())\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Fraction of Total Sales')\n    ax.grid(True, linestyle='--', alpha=0.6)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:42.933873Z","iopub.execute_input":"2025-01-24T09:21:42.934314Z","iopub.status.idle":"2025-01-24T09:21:42.942375Z","shell.execute_reply.started":"2025-01-24T09:21:42.93427Z","shell.execute_reply":"2025-01-24T09:21:42.941044Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Display country weights\n    print(\"Country Weights:\")\n    print(country_weights)\n\n    # Decompose and plot country fractions\n    fig, ax = plt.subplots(figsize=(10, 6))\n    decompose(train_df_imputed, 'country', ax, colormap='plasma')\n    ax.set_title(\"Country Fractions Over Time\")\n    plt.show()\n\n    # Display store weights\n    print(\"Store Weights:\")\n    print(store_weights)\n\n    # Decompose and plot store fractions\n    fig, ax = plt.subplots(figsize=(10, 6))\n    decompose(train_df_imputed, 'store', ax, colormap='plasma')\n    ax.set_title(\"Store Fractions Over Time\")\n    plt.show()\n\n    # Display product weights\n    print(\"Product Weights:\")\n    print(product_weights)\n\n    # Decompose and plot product fractions\n    fig, ax = plt.subplots(figsize=(10, 6))\n    decompose(train_df_imputed, 'product', ax, colormap='plasma')\n    ax.set_title(\"Product Fractions Over Time\")\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:42.943694Z","iopub.execute_input":"2025-01-24T09:21:42.944027Z","iopub.status.idle":"2025-01-24T09:21:44.420431Z","shell.execute_reply.started":"2025-01-24T09:21:42.943996Z","shell.execute_reply":"2025-01-24T09:21:44.419341Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Product Ratios and Forecasting\nCalculating normalized sales ratios for each product and forecasting future trends based on past data.","metadata":{}},{"cell_type":"code","source":"def forecast_product_ratios(train_df, forecast_years):\n    \"\"\"\n    Forecast product ratios for specific years based on historical data.\n\n    \"\"\"\n    # Calculate product fractions by day\n    product_df = train_df.groupby([\"date\", \"product\"])[\"num_sold\"].sum().reset_index()\n    \n    # Pivot to get a DataFrame where each column represents a product\n    product_ratio_df = product_df.pivot(index=\"date\", columns=\"product\", values=\"num_sold\")\n    \n    # Normalize each row to calculate product ratios\n    product_ratio_df = product_ratio_df.div(product_ratio_df.sum(axis=1), axis=0)\n    product_ratio_df = product_ratio_df.stack().rename(\"ratios\").reset_index()\n    \n    # Forecast product ratios for the given years\n    forecasted_ratios = []\n    for base_year, target_year, year_shift in forecast_years:\n        # Filter data for the base year\n        forecast_df = product_ratio_df[product_ratio_df[\"date\"].dt.year == base_year].copy()\n        # Shift the date to the target year\n        forecast_df[\"date\"] += pd.DateOffset(years=year_shift)\n        forecasted_ratios.append(forecast_df)\n    \n    forecasted_ratios_df = pd.concat(forecasted_ratios, ignore_index=True)\n    \n    return forecasted_ratios_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:44.421473Z","iopub.execute_input":"2025-01-24T09:21:44.421757Z","iopub.status.idle":"2025-01-24T09:21:44.429017Z","shell.execute_reply.started":"2025-01-24T09:21:44.421732Z","shell.execute_reply":"2025-01-24T09:21:44.427736Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* **Date Offsets:**\n\n    * For 2017, offset by 2 years from 2015.\n    * For 2018, offset by 2 years from 2016.\n    * For 2019, offset by 4 years from 2015.\n\n* **Output**\nThe `forecasted_ratios_df` will contain the product ratios for:\n\n    * 2017 (based on 2015 data).\n    * 2018 (based on 2016 data).\n    * 2019 (based on 2015 data).","metadata":{}},{"cell_type":"code","source":"# Define the input DataFrame (train_df_imputed) and forecast years\nforecast_years = [(2015, 2017, 2), (2016, 2018, 2), (2015, 2019, 4)]\n\nforecasted_ratios_df = forecast_product_ratios(train_df_imputed, forecast_years)\n\n# Display a sample of the forecasted ratios\nprint(\"Forecasted Product Ratios (Sample):\")\nprint(forecasted_ratios_df.head(5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:44.430055Z","iopub.execute_input":"2025-01-24T09:21:44.430433Z","iopub.status.idle":"2025-01-24T09:21:44.498661Z","shell.execute_reply.started":"2025-01-24T09:21:44.430397Z","shell.execute_reply":"2025-01-24T09:21:44.497509Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Time Series Aggregation for Total Sales\nAggregating sales data by time periods (date, year, month, day of the week) for feature extraction.","metadata":{}},{"cell_type":"code","source":"# Create a copy of the DataFrame\noriginal_train_df_imputed = train_df_imputed.copy()\n\n# Aggregate total sales by date\ntrain_df_imputed = train_df_imputed.groupby([\"date\"])[\"num_sold\"].sum().reset_index()\n\n# Extract year, month, day, and day of the week from the date\ntrain_df_imputed[\"year\"] = train_df_imputed[\"date\"].dt.year\ntrain_df_imputed[\"month\"] = train_df_imputed[\"date\"].dt.month\ntrain_df_imputed[\"day\"] = train_df_imputed[\"date\"].dt.day\ntrain_df_imputed[\"day_of_week\"] = train_df_imputed[\"date\"].dt.dayofweek\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:44.529164Z","iopub.execute_input":"2025-01-24T09:21:44.529496Z","iopub.status.idle":"2025-01-24T09:21:44.565813Z","shell.execute_reply.started":"2025-01-24T09:21:44.529458Z","shell.execute_reply":"2025-01-24T09:21:44.5647Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Correlation Analysis","metadata":{}},{"cell_type":"code","source":"correlation_features = [\n    'year', \n    'month', \n    'day', \n    'day_of_week', \n    'num_sold'\n]\n\n# Compute the correlation matrix\ncorrelation_matrix = train_df_imputed[correlation_features].corr()\n\nplt.figure(figsize=(7, 5))\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"plasma\", cbar=True, square=True, linewidths=0.5)\nplt.title(\"Correlation Heatmap of Features\", fontsize=12)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:44.566994Z","iopub.execute_input":"2025-01-24T09:21:44.567398Z","iopub.status.idle":"2025-01-24T09:21:44.904223Z","shell.execute_reply.started":"2025-01-24T09:21:44.567364Z","shell.execute_reply":"2025-01-24T09:21:44.903223Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The heatmap visualizes the correlation between different features in the dataset: `year`, `month`, `day`, `day_of_week`, and `num_sold`.\n\n**Specific Insights:**\n\n* **`num_sold` vs. `day_of_week`:** There is a moderate positive correlation (0.58) between the number of items sold (`num_sold`) and the day of the week. This suggests that sales activity might be influenced by the day of the week.\n\n* **`num_sold` vs. `year`:** A weak negative correlation (-0.27) exists between `num_sold` and `year`. This could indicate a slight decrease in sales over the years represented in the dataset. However, the correlation is weak, so further investigation is needed to confirm this trend.\n\n* **Other correlations:**\n    * `month` and `day` have a very weak positive correlation (0.01).\n    * `day_of_week` has a moderate positive correlation (0.58) with itself.\n    * All other correlations are very weak or negligible.\n\n**Possible Implications:**\n\n* **Day-of-the-week effect:** The moderate positive correlation between `num_sold` and `day_of_week` suggests that sales might be higher on certain days of the week. Further analysis could reveal which days have the highest and lowest sales.\n* **Year-over-year trend:** The weak negative correlation between `num_sold` and `year` might indicate a slight decline in sales over time. However, more data and analysis are needed to confirm this trend and understand the underlying reasons.","metadata":{}},{"cell_type":"markdown","source":"## Monthly Sales Trends\nAggregating sales by year and month and visualizing monthly trends.","metadata":{}},{"cell_type":"code","source":"# Aggregate total sales by year and month\nmonthly_sales = train_df_imputed.groupby(['year', 'month'])['num_sold'].sum().reset_index()\n\n# Create a 'date' column for monthly aggregation\nmonthly_sales['date'] = pd.to_datetime(monthly_sales[['year', 'month']].assign(day=1))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:44.905292Z","iopub.execute_input":"2025-01-24T09:21:44.905571Z","iopub.status.idle":"2025-01-24T09:21:44.919283Z","shell.execute_reply.started":"2025-01-24T09:21:44.905546Z","shell.execute_reply":"2025-01-24T09:21:44.918152Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set up the line plot for monthly sales trends\nplt.figure(figsize=(14, 7))\nsns.lineplot(\n    data=monthly_sales,\n    x='date',\n    y='num_sold',\n    marker='o',\n    color=sns.color_palette(\"plasma\", as_cmap=True)(0.5) \n)\n\nplt.title(\"Monthly Sales Trends\", fontsize=14)\nplt.xlabel(\"Month\", fontsize=12)\nplt.ylabel(\"Total Sales\", fontsize=12)\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:44.920393Z","iopub.execute_input":"2025-01-24T09:21:44.920692Z","iopub.status.idle":"2025-01-24T09:21:45.362324Z","shell.execute_reply.started":"2025-01-24T09:21:44.920665Z","shell.execute_reply":"2025-01-24T09:21:45.36122Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The plot visualizes the trend of monthly total sales over a period from 2010 to 2017.\nThe sales figures appear to fluctuate significantly over time.\n\n**Specific Insights:**\n\n* **Volatility:** The plot shows a high degree of volatility in monthly sales. There are periods of rapid increases followed by sharp declines, indicating that sales are not stable.\n* **No clear trend:** It's difficult to identify a consistent upward or downward trend in sales over the entire period. The fluctuations seem to be random rather than following a clear pattern.\n* **Potential Seasonality:** While not immediately apparent, there might be some subtle seasonal patterns within the volatility. For example, there could be slight increases or decreases in sales during certain months or quarters.","metadata":{}},{"cell_type":"markdown","source":"## Sales Trends Heatmap\nCreating a heatmap to show sales distribution across years and months.","metadata":{}},{"cell_type":"code","source":"# Pivot the data for heatmap\nheatmap_data = monthly_sales.pivot(index=\"year\", columns=\"month\", values=\"num_sold\")\n\nplt.figure(figsize=(12, 8))\nsns.heatmap(\n    heatmap_data,\n    annot=True, fmt=\".0f\", cmap=\"plasma\", linewidths=0.5, cbar=True\n)\n\nplt.title(\"Sales Trends by Month and Year\", fontsize=14)\nplt.xlabel(\"Month\", fontsize=12)\nplt.ylabel(\"Year\", fontsize=12)\nplt.xticks(ticks=range(1, 13), labels=[\n    \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \n    \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"\n], rotation=45, fontsize=10)\nplt.yticks(fontsize=10)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:45.363336Z","iopub.execute_input":"2025-01-24T09:21:45.363633Z","iopub.status.idle":"2025-01-24T09:21:45.993425Z","shell.execute_reply.started":"2025-01-24T09:21:45.363607Z","shell.execute_reply":"2025-01-24T09:21:45.992041Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The heatmap visualizes monthly sales trends from 2010 to 2016.\n\n**Specific Insights:**\n\n* **Year-over-Year Trends:** \n    * There is a general upward trend in sales from 2010 to 2016. \n    * However, the growth rate seems to vary across years. Some years show more significant increases compared to others.\n\n* **Monthly Seasonality:** \n    * Sales appear to be higher in certain months throughout the years. \n    * December consistently shows the highest sales volumes, suggesting a strong year-end seasonality.\n    * Other months with relatively high sales include November and October, possibly indicating pre-holiday shopping trends.\n\n* **Month-to-Month Variation:** \n    * Within each year, there is substantial variation in sales across different months. \n    * This suggests that factors beyond just year-over-year trends are influencing sales performance.\n\n**Possible Implications:**\n\n* **Seasonality Management:** The strong December sales suggest that businesses should prepare for increased demand and inventory management during this period.\n* **Marketing Strategies:** Targeted marketing campaigns during peak sales months (December, November, October) could help capitalize on the increased demand.\n* **Inventory Planning:** Understanding the seasonal patterns can help businesses optimize inventory levels throughout the year, avoiding stockouts during peak demand periods and minimizing excess inventory during slower months.","metadata":{}},{"cell_type":"markdown","source":"## Preparing Test Data for Forecasting\nAdding new columns like month, day, and day_of_week to facilitate downstream analysis and forecasting.","metadata":{}},{"cell_type":"code","source":"test_total_sales_df = test_df.groupby([\"date\"])[\"id\"].first().reset_index().drop(columns=\"id\")\ntest_total_sales_df[\"month\"] = test_total_sales_df[\"date\"].dt.month\ntest_total_sales_df[\"day\"] = test_total_sales_df[\"date\"].dt.day\ntest_total_sales_df[\"day_of_week\"] = test_total_sales_df[\"date\"].dt.dayofweek\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:45.994734Z","iopub.execute_input":"2025-01-24T09:21:45.995489Z","iopub.status.idle":"2025-01-24T09:21:46.009942Z","shell.execute_reply.started":"2025-01-24T09:21:45.995411Z","shell.execute_reply":"2025-01-24T09:21:46.008493Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Calculating Weekly Sales Ratios\nCalculates the daily sales ratios as a percentage of the weekly total sales. It also aggregates the weekly sales data and adds helpful columns for week identification and day-wise ratios.","metadata":{}},{"cell_type":"code","source":"df = train_df_imputed.copy()\ndf['iso_year'] = df['date'].dt.isocalendar().year \ndf['iso_week'] = df['date'].dt.isocalendar().week \ndf['week_id'] = df['iso_year'].astype(str) + '-W' + df['iso_week'].astype(str).str.zfill(2)\ndf['day_of_week'] = df['date'].dt.dayofweek\n\n# Aggregate weekly total sales\nweekly_total = df.groupby('week_id')['num_sold'].sum().reset_index()\nweekly_total.rename(columns={'num_sold': 'weekly_total_sold'}, inplace=True)\n\n# Merge weekly totals back to the original DataFrame\ndf = pd.merge(df, weekly_total, on='week_id')\n\n# Calculate daily sales ratio as a percentage of weekly total sales\ndf['daily_sales_ratio'] = df['num_sold'] / df['weekly_total_sold']\n\n# Aggregate weekly ratios by day of the week\nweekly_ratio = df.groupby(['week_id', 'day_of_week'])['daily_sales_ratio'].sum().reset_index()\n\n# Get the start date for each week (Monday)\nweekly_ratio['week_start'] = pd.to_datetime(weekly_ratio['week_id'] + '-1', format='%Y-W%W-%w')\nfirst_monday = weekly_ratio['week_start'].min()\n\n# Define days of the week for labeling in plots\ndays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:46.011234Z","iopub.execute_input":"2025-01-24T09:21:46.011655Z","iopub.status.idle":"2025-01-24T09:21:46.050601Z","shell.execute_reply.started":"2025-01-24T09:21:46.01161Z","shell.execute_reply":"2025-01-24T09:21:46.049287Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualization of Weekly Sales Ratio by Day of Week\nObserve the fluctuation of daily sales ratios over weeks. Each day of the week is plotted with a distinct color for better interpretation.","metadata":{}},{"cell_type":"code","source":"palette = sns.color_palette(\"plasma_r\", 7)  \n\nplt.figure(figsize=(14, 7))\nfor day in range(7):\n    day_data = weekly_ratio[weekly_ratio['day_of_week'] == day]\n    plt.plot(\n        day_data['week_start'], \n        day_data['daily_sales_ratio'], \n        label=days[day], \n        marker='o', \n        color=palette[day]  \n    )\n\nplt.xlim(left=first_monday)\nplt.ylim(top=0.3)\nplt.xlabel('Week Start Date (Monday)', fontsize=12)\nplt.ylabel('Daily Sales Ratio (Percentage of Weekly Total)', fontsize=12)\nplt.title('Weekly Sales Ratio by Day of Week', fontsize=14)\nplt.legend(title='Day of Week', fontsize=10)\nplt.grid(True, linestyle='--', alpha=0.6)  \nplt.xticks(rotation=45, fontsize=10)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:46.052275Z","iopub.execute_input":"2025-01-24T09:21:46.05273Z","iopub.status.idle":"2025-01-24T09:21:46.569219Z","shell.execute_reply.started":"2025-01-24T09:21:46.052687Z","shell.execute_reply":"2025-01-24T09:21:46.568089Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The plot visualizes the daily sales ratio (percentage of weekly total sales) for each day of the week over a period from 2010 to 2017.\nEach line represents a different day of the week (Monday to Sunday).\n\n**Specific Insights:**\n\n* **Weekday vs. Weekend Sales:** There is a clear distinction between weekday and weekend sales patterns.\n    * Weekdays (Monday to Friday) generally have lower sales ratios compared to weekends (Saturday and Sunday).\n    * This suggests that a significant portion of sales activity occurs during the weekend.\n      \n* **Day-to-Day Variations:** Even within weekdays, there are variations in sales ratios.\n    * Some weekdays consistently have lower sales ratios than others. For example,  Monday,Thursday, Wednesday and Thursday might have slightly lower sales compared to Friday.\n    * However, these variations are generally less pronounced compared to the difference between weekdays and weekends.\n\n* **Fluctuations Over Time:** There are some fluctuations in the sales ratios over the years.\n","metadata":{}},{"cell_type":"markdown","source":"## Total Sales by Day of the Week\nAggregates total sales by day of the week and showcases the sales distribution for each day.","metadata":{}},{"cell_type":"code","source":"# Aggregate sales by day of the week\nday_of_week_sales = train_df_imputed.groupby(\"day_of_week\")[\"num_sold\"].sum()\nday_of_week_sales.index = days  \n\nplt.figure(figsize=(10, 5))\nsns.barplot(x=day_of_week_sales.index, y=day_of_week_sales.values, palette=\"plasma_r\")\nplt.title(\"Total Sales by Day of the Week\", fontsize=14)\nplt.xlabel(\"Day of the Week\", fontsize=12)\nplt.ylabel(\"Total Sales\", fontsize=12)\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:46.570246Z","iopub.execute_input":"2025-01-24T09:21:46.570566Z","iopub.status.idle":"2025-01-24T09:21:46.844846Z","shell.execute_reply.started":"2025-01-24T09:21:46.570527Z","shell.execute_reply":"2025-01-24T09:21:46.843607Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The plot visualizes the total sales volume for each day of the week.\nThere is a clear upward trend in sales from Monday to Sunday.\n\n**Specific Insights:**\n\n* **Sunday Sales Dominance:** Sunday has the highest total sales among all days of the week.\n* **Weekday Sales Patterns:** Weekday sales generally increase from Monday to Friday, with a noticeable jump in sales from Friday to Saturday.\n* **Weekend Peak:** The significant increase in sales from Friday to Saturday and then to Sunday indicates a strong weekend sales trend.\n\n**Possible Implications:**\n\n* **Customer Behavior:** This pattern suggests that a significant portion of the customer base prefers to shop on weekends. This could be due to factors like leisure time, family time, or weekend promotions.\n* **Staffing and Scheduling:** Businesses can optimize staffing levels and scheduling based on expected sales volumes on different days of the week. More staff may be needed on weekends to handle the increased customer traffic and sales.\n* **Marketing and Promotions:** Targeted marketing campaigns or promotions could be implemented on weekdays to encourage more sales during the week.\n* **Inventory Management:** Understanding the weekend-focused sales can help businesses manage inventory levels to ensure sufficient stock availability on weekends and minimize excess inventory on weekdays.","metadata":{}},{"cell_type":"markdown","source":"## Sales Distribution Across Days and Months (Heatmap)","metadata":{}},{"cell_type":"code","source":"# Group by month and day of the week, summing the sales\nmonthly_day_sales = df.groupby(['month', 'day_of_week'])['num_sold'].sum().reset_index()\n\n# Define days of the week for labeling in plots\ndays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\n# Pivot the data for heatmap: rows = days, columns = months, values = sales\nheatmap_data = monthly_day_sales.pivot(index='day_of_week', columns='month', values='num_sold')\n\n# Set day names for rows; ensure index aligns with expected day order\nheatmap_data.index = days[:len(heatmap_data)]  \n\nplt.figure(figsize=(12, 8))\nsns.heatmap(\n    heatmap_data, \n    annot=True, fmt=\".0f\", cmap=\"plasma\", linewidths=0.5, cbar=True\n)\n\nplt.title(\"Aggregate Sales by Month and Day of Week\", fontsize=14)\nplt.xlabel(\"Month\", fontsize=12)\nplt.ylabel(\"Day of the Week\", fontsize=12)\nplt.xticks(ticks=range(1, 13), labels=[\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \n                                          \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"], rotation=45, fontsize=10)\nplt.yticks(fontsize=10)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:46.845877Z","iopub.execute_input":"2025-01-24T09:21:46.846224Z","iopub.status.idle":"2025-01-24T09:21:47.501633Z","shell.execute_reply.started":"2025-01-24T09:21:46.846191Z","shell.execute_reply":"2025-01-24T09:21:47.500214Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The heatmap visualizes the total sales volume for each combination of month and day of the week.\nColor intensity represents the total sales volume.\n\n**Specific Insights:**\n\n* **Day of the Week:**\n    * **Sunday** consistently shows the highest sales volumes across all months.\n    * Sales generally increase from Monday to Sunday, with a significant jump from Friday to Saturday.\n    * Weekday sales (Monday to Friday) are relatively lower compared to weekends.\n\n* **Monthly Trends:**\n    * December consistently shows the highest sales across all days of the week.\n    * November and October also tend to have high sales volumes.\n    * January typically shows lower sales compared to other months.\n\n* **Combined Trends:**\n    * The combination of Sunday and December shows the highest sales volume.\n    * Weekday sales are generally lower, with Monday and Tuesday showing the lowest sales volumes.\n","metadata":{}},{"cell_type":"markdown","source":"## Calculating Day of Week Ratios and Adjusting Sales","metadata":{}},{"cell_type":"code","source":"# Calculate Day of Week Ratios and Adjust Sales\n\n# Calculate the average sales for each day of the week and normalize it\nday_of_week_ratio = (\n    train_df_imputed.groupby(\"day_of_week\")[\"num_sold\"].mean() / \n    train_df_imputed.groupby(\"day_of_week\")[\"num_sold\"].mean().mean()\n).rename(\"day_of_week_ratios\")\n\n# Display day of week ratios\ndisplay(day_of_week_ratio)\n\n# Merge day of week ratios back into the main DataFrame\ntrain_df_imputed = pd.merge(train_df_imputed, day_of_week_ratio, how=\"left\", on=\"day_of_week\")\n\n# Adjust num_sold based on day of week ratios\ntrain_df_imputed[\"adjusted_num_sold\"] = train_df_imputed[\"num_sold\"] / train_df_imputed[\"day_of_week_ratios\"]\n\n# Check the difference for Adjusted Sales\ndifference_check = (train_df_imputed[\"num_sold\"].sum() - train_df_imputed[\"adjusted_num_sold\"].sum()) / train_df_imputed[\"num_sold\"].sum()\nprint(f\"The difference between original and adjusted total sales as a proportion is: {difference_check:.6f}\")\n\n# Print adjusted_num_sold values\nprint(\"\\nAdjusted Sales (Adjusted num_sold):\")\nprint(train_df_imputed[[\"date\", \"num_sold\", \"adjusted_num_sold\"]].head())  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:47.502688Z","iopub.execute_input":"2025-01-24T09:21:47.503106Z","iopub.status.idle":"2025-01-24T09:21:47.526199Z","shell.execute_reply.started":"2025-01-24T09:21:47.503062Z","shell.execute_reply":"2025-01-24T09:21:47.524801Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Day-of-Week Effect:**\n\n* **Weekdays (Monday-Thursday):** Sales on these days are generally lower than the average, with ratios consistently around 0.94. This suggests that sales activity is subdued during the weekdays.\n* **Friday:** Sales on Friday are close to the average, with a ratio near 1. This indicates that sales on Fridays are relatively typical compared to the overall average sales across all days.\n* **Saturday:** Sales on Saturday show an increase compared to the average, with a ratio of 1.056. This suggests that sales activity picks up on Saturdays.\n* **Sunday:** Sales on Sunday are significantly higher than the average, with the highest ratio of 1.169. This indicates that Sundays are the busiest sales days.\n\n**These ratios demonstrate a clear weekend sales trend, with sales increasing from weekdays to Saturday and peaking on Sunday.** \n\n* **Minimal Impact of Adjustment:** The very small difference between the original and adjusted total sales (0.000022) indicates that the adjustment based on day-of-week ratios has a negligible impact on the overall sales volume. This suggests that the average sales across different days of the week are relatively balanced.","metadata":{}},{"cell_type":"markdown","source":"## Making the Forecast\nPrepare test data for forecasting by merging the adjusted daily sales averages and day-of-week ratios into the test dataset. This enables accurate forecast generation.\n","metadata":{}},{"cell_type":"code","source":"# Function to Prepare Test Data for Forecasting\ndef prepare_test_data(train_df_imputed, test_total_sales_df, day_of_week_ratio):\n    \"\"\"\n    Prepare the test data by calculating daily mean sales and incorporating day-of-week ratios.\n    \n    \"\"\"\n    # Filter training data for the last X years (from 2010 onwards)\n    train_last_x_years_df = train_df_imputed.loc[train_df_imputed[\"year\"] >= 2010]\n    \n    # Calculate daily mean of adjusted sales for each month and day\n    train_day_mean_df = train_last_x_years_df.groupby([\"month\", \"day\"])[\"adjusted_num_sold\"].mean().reset_index()\n    \n    # Merge average daily sales into the test DataFrame\n    test_total_sales_df = pd.merge(test_total_sales_df, train_day_mean_df, how=\"left\", on=[\"month\", \"day\"])\n    \n    # Merge with day-of-week ratios\n    test_total_sales_df = pd.merge(test_total_sales_df, day_of_week_ratio.reset_index(), how=\"left\", on=\"day_of_week\")\n    \n    # Calculate forecasted daily sales\n    test_total_sales_df[\"num_sold\"] = test_total_sales_df[\"adjusted_num_sold\"] * test_total_sales_df[\"day_of_week_ratios\"]\n    \n    return test_total_sales_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:47.527223Z","iopub.execute_input":"2025-01-24T09:21:47.52758Z","iopub.status.idle":"2025-01-24T09:21:47.54208Z","shell.execute_reply.started":"2025-01-24T09:21:47.527541Z","shell.execute_reply":"2025-01-24T09:21:47.540552Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Disaggregating Total Sales Forecast\nDisaggregates the total sales forecast by combining store, country, and product-level ratios with the daily sales forecast, producing fine-grained sales predictions.\n","metadata":{}},{"cell_type":"code","source":"# Function to Disaggregate Total Sales Forecast\ndef disaggregate_forecast(test_df, test_total_sales_df, store_weights, gdp_per_capita_filtered_ratios_df, forecasted_ratios_df):\n    \"\"\"\n    Disaggregate total sales forecast by incorporating store, country, and product ratios.\n    \n    \"\"\"\n    # Add store ratios\n    store_weights_df = store_weights.reset_index()\n    test_sub_df = pd.merge(test_df, test_total_sales_df, how=\"left\", on=\"date\")\n    test_sub_df.rename(columns={\"num_sold\": \"day_num_sold\"}, inplace=True)\n    \n    # Add product ratios\n    test_sub_df = pd.merge(test_sub_df, store_weights_df, how=\"left\", on=\"store\")\n    test_sub_df.rename(columns={\"num_sold\": \"store_ratio\"}, inplace=True)\n    \n    # Add country ratios\n    test_sub_df[\"year\"] = test_sub_df[\"date\"].dt.year\n    test_sub_df = pd.merge(test_sub_df, gdp_per_capita_filtered_ratios_df, how=\"left\", on=[\"year\", \"country\"])\n    test_sub_df.rename(columns={\"ratio\": \"country_ratio\"}, inplace=True)\n    \n    # Add product ratios\n    test_sub_df = pd.merge(test_sub_df, forecasted_ratios_df, how=\"left\", on=[\"date\", \"product\"])\n    test_sub_df.rename(columns={\"ratios\": \"product_ratio\"}, inplace=True)\n    \n    # Adjust for bias for Kenya's GDP ratio\n    test_sub_df.loc[test_sub_df['country'] == 'Kenya', 'country_ratio'] += 0.00249144564 * 1 / 10\n    \n    # Calculate final forecasted `num_sold`\n    test_sub_df[\"num_sold\"] = (\n        test_sub_df[\"day_num_sold\"] * \n        test_sub_df[\"store_ratio\"] * \n        test_sub_df[\"country_ratio\"] * \n        test_sub_df[\"product_ratio\"]\n    )\n    \n    # Round `num_sold` to nearest integer\n    test_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].round()\n    \n    return test_sub_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:47.543283Z","iopub.execute_input":"2025-01-24T09:21:47.543604Z","iopub.status.idle":"2025-01-24T09:21:47.565427Z","shell.execute_reply.started":"2025-01-24T09:21:47.543574Z","shell.execute_reply":"2025-01-24T09:21:47.563965Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Plotting Individual Time Series\nGenerates time series plots for individual categories, enabling a side-by-side comparison of actual and forecasted trends.","metadata":{}},{"cell_type":"code","source":"# Function to Plot Individual Time Series\ndef plot_individual_ts(df):\n    \"\"\"\n    Plot individual time series for each combination of country, store, and product.\n    \n    \"\"\"\n    # Generate a color palette using Plasma\n    unique_countries = df[\"country\"].unique()\n    colour_map = sns.color_palette(\"plasma\", len(unique_countries))\n    \n    country_color_map = {country: colour_map[i] for i, country in enumerate(unique_countries)}\n    \n    for country in unique_countries:\n        f, axes = plt.subplots(df[\"store\"].nunique() * df[\"product\"].nunique(), figsize=(20, 70))\n        count = 0\n        \n        for store in df[\"store\"].unique():\n            for product in df[\"product\"].unique():\n                plot_data = df.loc[\n                    (df[\"product\"] == product) & \n                    (df[\"country\"] == country) & \n                    (df[\"store\"] == store)\n                ]\n                sns.lineplot(data=plot_data, x=\"date\", y=\"num_sold\", linewidth=0.5,\n                             ax=axes[count], color=country_color_map[country])\n                axes[count].set_title(f\"{country} - {store} - {product}\")\n                axes[count].axvline(pd.to_datetime(\"2017-01-01\"), color='black', linestyle='--')\n                axes[count].grid(True, linestyle='--', alpha=0.6)\n                count += 1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:47.568032Z","iopub.execute_input":"2025-01-24T09:21:47.568544Z","iopub.status.idle":"2025-01-24T09:21:47.598056Z","shell.execute_reply.started":"2025-01-24T09:21:47.568499Z","shell.execute_reply":"2025-01-24T09:21:47.59688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare the forecasted data\ntest_total_sales_forecasted = prepare_test_data(train_df_imputed, test_total_sales_df, day_of_week_ratio)\n\n# Disaggregate the forecasted data\ntest_forecast_disaggregated = disaggregate_forecast(\n    test_df,\n    test_total_sales_forecasted,\n    store_weights,\n    gdp_per_capita_filtered_ratios_df,\n    forecasted_ratios_df\n)\n\n# Plot individual time series using both original and forecasted data\nplot_individual_ts(pd.concat([original_train_df_imputed, test_forecast_disaggregated]).reset_index(drop=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:21:47.599286Z","iopub.execute_input":"2025-01-24T09:21:47.599593Z","iopub.status.idle":"2025-01-24T09:25:03.463495Z","shell.execute_reply.started":"2025-01-24T09:21:47.599557Z","shell.execute_reply":"2025-01-24T09:25:03.46178Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creating the Submission File","metadata":{}},{"cell_type":"code","source":"# Create submission file\nsample_df = pd.read_csv(\"/kaggle/input/playground-series-s5e1/sample_submission.csv\")\nsample_df[\"num_sold\"] = test_forecast_disaggregated[\"num_sold\"]\n\ndisplay(sample_df.head(5))\n\nsample_df.to_csv(\"submission.csv\", index=False)\nprint(\"Submission file created.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T09:25:03.465108Z","iopub.execute_input":"2025-01-24T09:25:03.465618Z","iopub.status.idle":"2025-01-24T09:25:03.644614Z","shell.execute_reply.started":"2025-01-24T09:25:03.465558Z","shell.execute_reply":"2025-01-24T09:25:03.643246Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<!-- Include Google Fonts for a modern font -->\n<link href=\"https://fonts.googleapis.com/css2?family=Roboto:wght@700&display=swap\" rel=\"stylesheet\">\n\n<div style=\"border-radius: 15px; border: 2px solid #8455a1; padding: 20px; \n           background: linear-gradient(135deg, #fcaed5, #fabba2); \n           text-align: left; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.5);\">\n    <div style=\"background-color: rgba(255, 255, 255, 0.2); padding: 20px; border-radius: 10px;\">\n        <h3 style=\"color: #6A1B9A; font-weight: bold; margin-bottom: 10px; font-size: 18px; font-family: 'Roboto', sans-serif; line-height: 1.2;\">\n            References :\n        </h3>\n        <p style=\"color: #8455a1; font-size: 16px; margin-bottom: 10px;\">\n            [S5E1] Previous Years Baseline - No Model: \n            <a href=\"https://www.kaggle.com/code/cabaxiom/s5e1-previous-years-baseline-no-model\" style=\"color: #8455a1; text-decoration: underline;\">https://www.kaggle.com/code/cabaxiom/s5e1-previous-years-baseline-no-model</a>\n        </p>\n        <p style=\"color: #8455a1; font-size: 16px; margin-bottom: 10px;\">\n            [S5E1] EDA and Linear Regression Baseline: \n            <a href=\"https://www.kaggle.com/code/cabaxiom/s5e1-eda-and-linear-regression-baseline?scriptVersionId=216127469\" style=\"color: #8455a1; text-decoration: underline;\">https://www.kaggle.com/code/cabaxiom/s5e1-eda-and-linear-regression-baseline</a>\n        </p>\n        <p style=\"color: #8455a1; font-size: 16px; margin-bottom: 10px;\">\n            EDA and using previous ratio as prediction: \n            <a href=\"https://www.kaggle.com/code/act18l/eda-and-using-previous-ratio-as-prediction\" style=\"color: #8455a1; text-decoration: underline;\">https://www.kaggle.com/code/act18l/eda-and-using-previous-ratio-as-prediction</a>\n        </p>\n        <p style=\"color: #8455a1; font-size: 16px;\">\n            Discussion Forums:\n            <a href=\"https://www.kaggle.com/competitions/playground-series-s5e1/discussion/554349\" style=\"color: #8455a1; text-decoration: underline;\">https://www.kaggle.com/competitions/playground-series-s5e1/discussion/554349</a>,\n            <a href=\"https://www.kaggle.com/competitions/playground-series-s5e1/discussion/555500\" style=\"color: #8455a1; text-decoration: underline;\">https://www.kaggle.com/competitions/playground-series-s5e1/discussion/555500</a>\n        </p>\n        <p style=\"color: #8455a1; font-size: 16px;\">\n            Competition: \n            <a href=\"https://kaggle.com/competitions/playground-series-s5e1\" style=\"color: #8455a1; text-decoration: underline;\">Playground Series S5E1: Forecasting Sticker Sales</a> by Walter Reade and Elizabeth Park.\n        </p>\n    </div>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<!-- Include Google Fonts for a modern font -->\n<link href=\"https://fonts.googleapis.com/css2?family=Roboto:wght@700&display=swap\" rel=\"stylesheet\">\n\n<div style=\"border-radius: 15px; border: 2px solid #6A1B9A; padding: 20px; \n           background: linear-gradient(135deg, #6610f2, #ff99cc, #ffa07a, #ffff00); \n           text-align: center; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.5);\">\n    <h1 style=\"color: #ffffff; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7); \n               font-weight: bold; margin-bottom: 10px; font-size: 28px; \n               font-family: 'Roboto', sans-serif; line-height: 1.2;\">\n        🙏 Thanks for Reading! 🚀\n    </h1>\n    <p style=\"color: #ffffff; font-size: 22px; text-align: center;\">\n        Happy Coding! 🙌😊\n    </p>\n</div>\n","metadata":{}}]}